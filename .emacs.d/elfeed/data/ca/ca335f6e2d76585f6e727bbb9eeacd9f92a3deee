<h1 id="nginx性能和软中断"><a href="#nginx性能和软中断" class="headerlink" title="nginx性能和软中断"></a>nginx性能和软中断</h1><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><ul>
<li>如何调整软中断才能达到最优性能？</li>
<li>通过 top 观察软中断 和 si%、sy% 的关系</li>
</ul>
<h2 id="测试机型"><a href="#测试机型" class="headerlink" title="测试机型"></a>测试机型</h2><p>双路 Intel(R) Xeon(R) CPU E5-2682 v4</p>
<p>两块万兆网卡：Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)</p>
<p>内核：3.10.0-327</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">NUMA node0 CPU(s):     0-15,32-47</div><div class="line">NUMA node1 CPU(s):     16-31,48-63</div></pre></td></tr></table></figure>
<h2 id="软中断和-si"><a href="#软中断和-si" class="headerlink" title="软中断和 si%"></a>软中断和 si%</h2><p>压nginx 碰到一个奇怪的问题，将软中断绑到48-63核，如果nginx绑到这个socket下的其它核比如 16-23，我就基本上看不到 si% 的使用率；如果所有条件都不变我将nginx 绑0-7core（另外一个socket），那么我能看到0-7 core上的软中断 si%使用率达到600%以上（8core累加）。 si%使用率应该只和 PPS、流量相关，这个测试中不同绑核nginx的QPS 差了20%以内。</p>
<p><img src="/images/951413iMgBlog/image-20221031152031791.png" alt="image-20221031152031791"><img src="/images/951413iMgBlog/image-20221031152044825.png" alt="image-20221031152044825"></p>
<p>CPU是intel E5，网卡插在node0上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Model name:            Intel(R) Xeon(R) CPU E5-2682 v4 @ 2.50GHz</div><div class="line">NUMA node0 CPU(s):     0-15,32-47</div><div class="line">NUMA node1 CPU(s):     16-31,48-63</div><div class="line"></div><div class="line">软中断绑定：IRQBALANCE_BANNED_CPUS=0000ffff,ffffffff</div></pre></td></tr></table></figure>
<p>默认业务进程调用内核软中断do_softirq等来处理收发包，不需要跨core，如果将软中断绑定到具体的core后，会触发ksoftirqd 来调用do_softirq来处理收发包，整体上肯定效率不如同一个core处理业务和软中断的效率高。进一步如果软中断跨socket绑定导致处理时长进一步升高、总效率更差</p>
<p><a href="https://askubuntu.com/questions/7858/why-is-ksoftirqd-0-process-using-all-of-my-cpu" target="_blank" rel="external">https://askubuntu.com/questions/7858/why-is-ksoftirqd-0-process-using-all-of-my-cpu</a></p>
<p><img src="/images/951413iMgBlog/image-20221101113948809.png" alt="image-20221101113948809"></p>
<p>下图场景下，收包没有占用 si，而是占用的 sy</p>
<p><img src="/images/951413iMgBlog/image-20221101114217738.png" alt="image-20221101114217738"></p>
<p>将软中断和业务进程拆开绑核，均将软中断、业务基本压满的情况下，如果软中断在本node，QPS 增加20%+</p>
<p>软中断打满单核后的IPC：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">#perf stat --cpu 29  //软中断所在core，si%=100%，和业务以及网卡跨node</div><div class="line">^C</div><div class="line"> Performance counter stats for &apos;CPU(s) 29&apos;:</div><div class="line"></div><div class="line">       4470.584807      task-clock (msec)         #    1.001 CPUs utilized            (100.00%)</div><div class="line">               252      context-switches          #    0.056 K/sec                    (100.00%)</div><div class="line">                 8      cpu-migrations            #    0.002 K/sec                    (100.00%)</div><div class="line">                 3      page-faults               #    0.001 K/sec</div><div class="line">    11,158,106,237      cycles                    #    2.496 GHz                      (100.00%)</div><div class="line">   &lt;not supported&gt;      stalled-cycles-frontend</div><div class="line">   &lt;not supported&gt;      stalled-cycles-backend</div><div class="line">     7,976,745,525      instructions              #    0.71  insns per cycle          (100.00%)</div><div class="line">     1,444,740,326      branches                  #  323.166 M/sec                    (100.00%)</div><div class="line">         7,073,805      branch-misses             #    0.49% of all branches</div><div class="line"></div><div class="line">       4.465613433 seconds time elapsed</div><div class="line"></div><div class="line">#perf stat --cpu 1  //软中断所在core，si%=100%，和业务以及网卡跨node</div><div class="line">^C</div><div class="line">^C</div><div class="line"> Performance counter stats for &apos;CPU(s) 1&apos;:</div><div class="line"></div><div class="line">       5132.639092      task-clock (msec)         #    1.002 CPUs utilized            (100.00%)</div><div class="line">             1,119      context-switches          #    0.218 K/sec                    (100.00%)</div><div class="line">                 6      cpu-migrations            #    0.001 K/sec                    (100.00%)</div><div class="line">                 0      page-faults               #    0.000 K/sec</div><div class="line">    12,773,996,227      cycles                    #    2.489 GHz                      (100.00%)</div><div class="line">   &lt;not supported&gt;      stalled-cycles-frontend</div><div class="line">   &lt;not supported&gt;      stalled-cycles-backend</div><div class="line">    12,457,832,798      instructions              #    0.98  insns per cycle          (100.00%)</div><div class="line">     2,243,820,953      branches                  #  437.167 M/sec                    (100.00%)</div><div class="line">        12,769,358      branch-misses             #    0.57% of all branches</div><div class="line"></div><div class="line">       5.124937947 seconds time elapsed</div></pre></td></tr></table></figure>
<p>Nginx业务进程的IPC</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">#perf stat -p 30434   //软中断跨node</div><div class="line"></div><div class="line"> Performance counter stats for process id &apos;30434&apos;:</div><div class="line"></div><div class="line">       6838.088642      task-clock (msec)         #    0.953 CPUs utilized            (100.00%)</div><div class="line">            19,664      context-switches          #    0.003 M/sec                    (100.00%)</div><div class="line">                 0      cpu-migrations            #    0.000 K/sec                    (100.00%)</div><div class="line">                 4      page-faults               #    0.001 K/sec</div><div class="line">    17,027,659,259      cycles                    #    2.490 GHz                      (100.00%)</div><div class="line">   &lt;not supported&gt;      stalled-cycles-frontend</div><div class="line">   &lt;not supported&gt;      stalled-cycles-backend</div><div class="line">    14,315,679,297      instructions              #    0.84  insns per cycle          (100.00%)</div><div class="line">     2,919,774,303      branches                  #  426.987 M/sec                    (100.00%)</div><div class="line">        34,643,571      branch-misses             #    1.19% of all branches</div><div class="line"></div><div class="line">       7.176493377 seconds time elapsed      </div><div class="line">       </div><div class="line">#perf stat -p 30434    //软中断和nginx、网卡在同一node</div><div class="line">^C</div><div class="line"> Performance counter stats for process id &apos;30434&apos;:</div><div class="line"></div><div class="line">       5720.308631      task-clock (msec)         #    0.979 CPUs utilized            (100.00%)</div><div class="line">            11,513      context-switches          #    0.002 M/sec                    (100.00%)</div><div class="line">                 1      cpu-migrations            #    0.000 K/sec                    (100.00%)</div><div class="line">                 0      page-faults               #    0.000 K/sec</div><div class="line">    14,234,226,577      cycles                    #    2.488 GHz                      (100.00%)</div><div class="line">   &lt;not supported&gt;      stalled-cycles-frontend</div><div class="line">   &lt;not supported&gt;      stalled-cycles-backend</div><div class="line">    14,741,777,543      instructions              #    1.04  insns per cycle          (100.00%)</div><div class="line">     3,009,021,477      branches                  #  526.024 M/sec                    (100.00%)</div><div class="line">        35,690,882      branch-misses             #    1.19% of all branches</div><div class="line"></div><div class="line">       5.845534744 seconds time elapsed</div></pre></td></tr></table></figure>
<p>如果将nginx绑到node1（和网卡分开），同样再将软中断绑到node0、node1上，这个时候同样是软中断和业务在同一node性能要好，也就是软中断要和业务在一个node和网卡在哪里没关系。</p>
<p>网络包收发涉及两块内存分配：描述符(指针)和data buffer（存放网络包数据）；</p>
<p><a href="https://ata.alibaba-inc.com/articles/230545" target="_blank" rel="external">网卡的描述符、data buffer申请的内存都在设备所在的numa上</a>， 如果将队列的中断绑定到其他cpu上， 那么队列申请的data buffer的节点也会跟着中断迁移，但是描述符是和网卡所在的node绑定不会迁移的。</p>
<p>Top 看到的 ksoftirqd 占用cpu不高，但是去看对应的 CPU core si消耗比较高，这是因为 ksoftirqd 只是触发软中断后的入口，进而会调用do_softirq/net_rx_action 等内核函数，在 si% 的消耗中包含了这些被调用的消耗</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li><p>软中断绑定优先让irqbalance自己决定，默认系统倾向于自动在业务中调用软中断，代价最低</p>
</li>
<li><p>尽量不要让包溢出net.core.netdev_budget，溢出后触发ksoftirqd 来处理效率更低</p>
</li>
<li><p>尽量控制不要让 ksoftirqd 打满，所以可以绑定更多core来</p>
</li>
</ul>
