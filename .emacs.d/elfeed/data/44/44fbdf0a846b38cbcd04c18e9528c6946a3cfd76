<h1 id="Linux-BUG内核导致的-TCP连接卡死"><a href="#Linux-BUG内核导致的-TCP连接卡死" class="headerlink" title="Linux BUG内核导致的 TCP连接卡死"></a>Linux BUG内核导致的 TCP连接卡死</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>客户端从 server 拖数据，偶尔会出现 TCP 连接卡死，卡死的现象就是 server 不遵循 TCP 重传逻辑，客户端不停地发 dup ack，但是服务端不响应这些dup ack仍然发新的包(从server抓包可以看到)，直至服务端不再发任何新包，最终连接闲置过久被reset，客户端抛连接异常.</p>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>服务端抓包可以看到：这个 TCP 流， 17:40:40 后 3306 端口不做任何响应，进入卡死状态，在卡死前有一些重传</p>
<p><img src="/images/951413iMgBlog/1662602586968-b20b6006-884e-4c33-9938-0277c012579e.png" alt="image.png"></p>
<p>同时通过观察这些连接的实时状态：</p>
<p><img src="/images/951413iMgBlog/image-20220922092105581.png" alt="image-20220922092105581"></p>
<p>rto一直在增加，但是这个时候 server 上抓不到任何包，说明内核在做 rto 重传，但是重传包没有到达本机网卡，应该还是被内核其它环节吃掉了。</p>
<p>再观察 netstat -s 状态，重传的时候，TCPWqueueTooBig 值会增加，也就是重传-&gt;TCPWqueueTooBig-&gt;重传包未发出-&gt;循环-&gt;相当于 TCP 连接卡死、静默状态</p>
<p><img src="/images/951413iMgBlog/image-20220922092321039.png" alt="image-20220922092321039"></p>
<p>顺着 TCPWqueueTooBig 查看<a href="https://github.com/torvalds/linux/commit/f070ef2ac66716357066b683fb0baf55f8191a2e" target="_blank" rel="external">内核代码提交记录</a>， 红色部分是修 CVE-2019-11478 添加的代码，引入了这个 卡死 的bug，绿色部分增加了更严格的条件又修复了卡死的 bug</p>
<p><img src="/images/951413iMgBlog/1662698955965-276e9936-6ca4-4269-9fbd-ae05176bf1a6.png" alt="image.png"></p>
<h2 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h2><p>2019-05 为了解决 <a href="https://www.secrss.com/articles/11570" target="_blank" rel="external">CVE-2019-11478</a> 增加了这个commit：<a href="https://github.com/torvalds/linux/commit/f070ef2ac66716357066b683fb0baf55f8191a2e" target="_blank" rel="external">f070ef2ac66716357066b683fb0baf55f8191a2e</a>，这部分代码在发送 buffer 满的时候忽略要发的包，进入静默</p>
<p>为了解决这个问题 2019-07-20 fix 版本：<a href="https://github.com/torvalds/linux/commit/b617158dc096709d8600c53b6052144d12b89fab" target="_blank" rel="external">https://github.com/torvalds/linux/commit/b617158dc096709d8600c53b6052144d12b89fab</a></p>
<p>4.19.57 是 2019-07-03 发布，完美引入了这个 bug</p>
<p>快速确认：netstat -s | grep TCPWqueueTooBig  如果不为0 就出现过 TCP 卡死，同时还可以看到 tb(待发送队列) 大于 rb（发送队列 buffer）</p>
<h2 id="重现条件"><a href="#重现条件" class="headerlink" title="重现条件"></a>重现条件</h2><p>必要条件：合并了 commit：<a href="https://github.com/torvalds/linux/commit/f070ef2ac66716357066b683fb0baf55f8191a2e" target="_blank" rel="external">f070ef2ac66716357066b683fb0baf55f8191a2e</a> 的内核版本</p>
<p>提高重现概率的其它非必要条件：</p>
<ol>
<li>数据量大—拖数据任务、大查询；</li>
<li>有丢包—链路偏长连接，丢包概率大；</li>
<li>多个任务 —一个失败整个任务失败，客户体感强烈</li>
<li>Server 设置了小buffer，出现概率更高</li>
</ol>
<p>在这四种情况下出现概率更高。用户单个小查询SQL 睬中这个bug后一般可能就是个连接异常，重试就过去了，所以可能没有抱怨。 得这四个条件一起用户的抱怨就会凸显出来。</p>
<h2 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h2><p>升级内核到带有2019-07-20 fix 版本：<a href="https://github.com/torvalds/linux/commit/b617158dc096709d8600c53b6052144d12b89fab" target="_blank" rel="external">https://github.com/torvalds/linux/commit/b617158dc096709d8600c53b6052144d12b89fab</a></p>
<h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><p><a href="https://www.secrss.com/articles/11570" target="_blank" rel="external">https://www.secrss.com/articles/11570</a></p>
<p><a href="https://access.redhat.com/solutions/4302501" target="_blank" rel="external">https://access.redhat.com/solutions/4302501</a></p>
<p><a href="https://access.redhat.com/solutions/5162381" target="_blank" rel="external">https://access.redhat.com/solutions/5162381</a></p>
<p>databricks 的相同案例： <a href="https://www.databricks.com/blog/2019/09/16/adventures-in-the-tcp-stack-performance-regressions-vulnerability-fixes.html" target="_blank" rel="external">https://www.databricks.com/blog/2019/09/16/adventures-in-the-tcp-stack-performance-regressions-vulnerability-fixes.html</a></p>
<p>6月第一个人报了这个bug：<a href="https://lore.kernel.org/netdev/CALMXkpYVRxgeqarp4gnmX7GqYh1sWOAt6UaRFqYBOaaNFfZ5sw@mail.gmail.com/" target="_blank" rel="external">https://lore.kernel.org/netdev/CALMXkpYVRxgeqarp4gnmX7GqYh1sWOAt6UaRFqYBOaaNFfZ5sw@mail.gmail.com/</a></p>
<blockquote>
<p>Hi Eric, I now have a packetdrill test that started failing (see below). Admittedly, a bit weird test with the SO_SNDBUF forced so low. Nevertheless, previously this test would pass, now it stalls after the write() because tcp_fragment() returns -ENOMEM. Your commit-message mentions that this could trigger when one sets SO_SNDBUF low. But, here we have a complete stall of the connection and we never recover.<br>I don’t know if we care about this, but there it is :-)</p>
</blockquote>
<p><a href="https://patches.linaro.org/project/stable/patch/20210125183204.684104321@linuxfoundation.org/" target="_blank" rel="external">一个 zero windows 下卡死的内核bug</a></p>
